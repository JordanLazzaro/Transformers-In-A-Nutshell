{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNxa5fI3qAxzUjZ1WQAffzF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/JordanLazzaro/81cf023d5d5478a5958cf885c8891504/transformersinanutshell.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers in a Nutshell\n",
        "An educational but usable example of a (character level) GPT-2 transformer language model.\n",
        "\n",
        "Feel free to play around with the dataset and hyperparams, the current ones are both chosen for ease of training and understandability."
      ],
      "metadata": {
        "id": "2LRRETL76Dly"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PXGA-IrOqx9"
      },
      "outputs": [],
      "source": [
        "!pip install -q wget wandb pytorch-lightning\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import wget\n",
        "from tqdm import tqdm\n",
        "\n",
        "# for logging metrics to wandb\n",
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "# for dataset\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "# for model\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchmetrics.functional import accuracy\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import WandbLogger"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Config:\n",
        "    \"\"\"\n",
        "    'gpt2-mini' config from minGPT\n",
        "    \"\"\"\n",
        "    # data\n",
        "    default_data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "    \n",
        "    # model\n",
        "    vocab_size = None\n",
        "    max_seq_len = 128\n",
        "    emb_size = 192\n",
        "    num_blocks = 6\n",
        "    num_heads = 6\n",
        "    fc_hidden_dim = 4 * emb_size\n",
        "    \n",
        "    # regularization\n",
        "    attn_dropout_p = 0.1\n",
        "    res_dropout_p = 0.1\n",
        "    emb_dropout_p = 0.1\n",
        "    \n",
        "    # training\n",
        "    max_learning_rate = 2.5e-4\n",
        "    batch_size = 512\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        \"\"\" any extra config args \"\"\"\n",
        "        for k, v in kwargs.items():\n",
        "            setattr(self, k, v)"
      ],
      "metadata": {
        "id": "hiJfSevUO2qA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharDataset(Dataset):\n",
        "    def __init__(self, config, data=None):\n",
        "        \"\"\"\n",
        "        A toy dataset class for charGPT modified from the minGPT repo\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        if data is None:\n",
        "            filename = wget.download(config.default_data_url)\n",
        "            data = open(filename, 'r').read()\n",
        "\n",
        "        chars = sorted(list(set(data)))\n",
        "        data_size, vocab_size = len(data), len(chars)\n",
        "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "\n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.config.max_seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        chunk = self.data[idx:idx + self.config.max_seq_len + 1]\n",
        "        # encode every character to an integer\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        # return as tensors\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        \n",
        "        return x, y"
      ],
      "metadata": {
        "id": "m_pedIh6PDxB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.emb_size % config.num_heads == 0\n",
        "\n",
        "        self.W_Q = nn.Linear(config.emb_size, config.emb_size, bias=False)\n",
        "        self.W_K = nn.Linear(config.emb_size, config.emb_size, bias=False)\n",
        "        self.W_V = nn.Linear(config.emb_size, config.emb_size, bias=False)\n",
        "        self.res_proj = nn.Linear(config.emb_size, config.emb_size, bias=False)\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(config.attn_dropout_p)\n",
        "        self.res_dropout = nn.Dropout(config.res_dropout_p)\n",
        "        \n",
        "        self.register_buffer(\n",
        "            'mask',\n",
        "            torch.tril(torch.ones(config.max_seq_len, config.max_seq_len))\n",
        "        )\n",
        "        \n",
        "        self.num_heads = config.num_heads\n",
        "\n",
        "    def forward(self, x):\n",
        "        # step 0) size: (b_s, s_l, e_s)\n",
        "        batch_size, seq_len, emb_size = x.size()\n",
        "        head_dim = emb_size // self.num_heads\n",
        "        \n",
        "        # step 1) size: (b_s, s_l, e_s) -> (b_s, s_l, n_h, h_d)\n",
        "        Q = self.W_Q(x).reshape(batch_size, seq_len, self.num_heads, head_dim)\n",
        "        K = self.W_K(x).reshape(batch_size, seq_len, self.num_heads, head_dim)\n",
        "        V = self.W_V(x).reshape(batch_size, seq_len, self.num_heads, head_dim)\n",
        "\n",
        "        # step 2) size: (b_s, s_l, n_h, h_d) -> (b_s, n_h, s_l, h_d)\n",
        "        Q = Q.transpose(1, 2)\n",
        "        K = K.transpose(1, 2)\n",
        "        V = V.transpose(1, 2)\n",
        "\n",
        "        # step 3) size: (b_s, n_h, s_l, h_d) x (b_s, n_h, h_d, s_l) = (b_s, n_h, s_l, s_l)\n",
        "        scores = Q @ K.transpose(-2, -1) * (1.0 / math.sqrt(head_dim))\n",
        "\n",
        "        # step 4) mask score values occuring ahead of a given element's position\n",
        "        scores = scores.masked_fill(self.mask[:seq_len, :seq_len]==0, float('-inf'))\n",
        "\n",
        "        # step 5) row-wise softmax (prob. dist. over values for every query)\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.attn_dropout(attn)\n",
        "\n",
        "        # step 6) size: (b_s, n_h, s_l, s_l) x (b_s, n_h, s_l, h_d) = (b_s, n_h, s_l, h_d)\n",
        "        out = attn @ V\n",
        "\n",
        "        # step 7) size: (b_s, n_h, s_l, h_d) -> (b_s, s_l, e_s)\n",
        "        out = out.transpose(1, 2).reshape(batch_size, seq_len, emb_size)\n",
        "        \n",
        "        # step 8) project concatentated heads into embedding space\n",
        "        out = self.res_proj(out)\n",
        "        out = self.res_dropout(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "-jMwDArhowby"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.hidden = nn.Linear(config.emb_size, config.fc_hidden_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.res_proj = nn.Linear(config.fc_hidden_dim, config.emb_size)\n",
        "        self.res_dropout = nn.Dropout(config.res_dropout_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.res_proj(x)\n",
        "        x = self.res_dropout(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "SRLVYVQ-ZHLD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.emb_size)\n",
        "        self.attn = CausalMultiHeadAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.emb_size)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ln1(x)\n",
        "        x = x + self.attn(x)\n",
        "        x = self.ln2(x)\n",
        "        x = x + self.mlp(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "ZejHjDOLgdtz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.tok_emb = nn.Embedding(dataset.vocab_size, config.emb_size)\n",
        "        self.pos_emb = nn.Embedding(config.max_seq_len, config.emb_size)\n",
        "        self.emb_dropout = nn.Dropout(config.emb_dropout_p)\n",
        "\n",
        "        self.blocks = nn.Sequential(*[TransformerBlock(config) for _ in range(config.num_blocks)])\n",
        "        self.ln = nn.LayerNorm(config.emb_size)\n",
        "        self.head = nn.Linear(config.emb_size, config.vocab_size, bias=False)\n",
        "        \n",
        "        # parameter with list of indices to slice into for retrieving pos_emb\n",
        "        self.pos_idxs = nn.Parameter(torch.arange(0, config.max_seq_len), requires_grad=False)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.size()\n",
        "        tok_embs = self.tok_emb(x)\n",
        "        pos_embs = self.pos_emb(self.pos_idxs[:seq_len])\n",
        "\n",
        "        seq = self.emb_dropout(tok_embs + pos_embs)\n",
        "        seq = self.blocks(seq)\n",
        "        seq = self.ln(seq)\n",
        "        out = self.head(seq)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "        \n",
        "        for name, param in self.named_parameters():\n",
        "            if name.endswith('res_proj.weight'):\n",
        "                # two residual connections per block (i.e. attn and mlp)\n",
        "                torch.nn.init.normal_(param, mean=0.0, std=0.02/math.sqrt(2 * config.num_blocks))"
      ],
      "metadata": {
        "id": "XsJ6vhlMhsCB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2LitModel(pl.LightningModule):\n",
        "    def __init__(self, model, config):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self.model(x)\n",
        "        loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
        "        self.log('train_loss', loss)\n",
        "        \n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.config.max_learning_rate)\n",
        "        \n",
        "        return optimizer"
      ],
      "metadata": {
        "id": "WGV1V6gnnNgh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wiring everything up to start training\n",
        "config = GPT2Config()\n",
        "dataset = CharDataset(config)\n",
        "config.vocab_size = dataset.vocab_size\n",
        "\n",
        "train_loader = DataLoader(dataset, num_workers=4, batch_size=config.batch_size, shuffle=True)\n",
        "\n",
        "model = GPT2(config)\n",
        "lit_model = GPT2LitModel(model, config)\n",
        "\n",
        "wandb_logger = WandbLogger()\n",
        "trainer = pl.Trainer(logger=wandb_logger, accelerator=\"gpu\", devices=1, max_epochs=10)\n",
        "\n",
        "# trainer without wandb logging\n",
        "# trainer = pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=10)"
      ],
      "metadata": {
        "id": "Qqri2b57nXNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(lit_model, train_loader)"
      ],
      "metadata": {
        "id": "vh1wx8RdBkUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save our trained model so we can use it later\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ls /content/gdrive/My\\ Drive\n",
        "\n",
        "model_save_name = 'shakespeareGPT.pt'\n",
        "path = f'/content/gdrive/My Drive/{model_save_name}'\n",
        "torch.save(lit_model.state_dict(), path)"
      ],
      "metadata": {
        "id": "dH6-ZDGl77qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simple helper function to prompt model and get readable result\n",
        "@torch.no_grad()\n",
        "def get_predictions(model, prompt, max_seq_len=128):\n",
        "    input = torch.LongTensor([dataset.stoi[i] for i in prompt]).unsqueeze(0)\n",
        "    while input.size(1) < max_seq_len:\n",
        "        logits = model(input)\n",
        "        logits = logits[:, -1, :]\n",
        "        probs = F.softmax(logits)\n",
        "        idxs = torch.multinomial(probs, num_samples=1)\n",
        "        input = torch.cat((input, idxs), dim=1)\n",
        "    \n",
        "    out_str = ''.join([dataset.itos[int(i)] for i in input[0].tolist()])\n",
        "\n",
        "    return out_str"
      ],
      "metadata": {
        "id": "p1Lct4OaJHC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading saved model to use for inference\n",
        "# from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ls /content/gdrive/My\\ Drive\n",
        "\n",
        "model_save_name = 'shakespeareGPT.pt'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
        "lit_model.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "id": "1_bWPMB48n6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Who art thou?' # put your propmt here!\n",
        "preds_str = get_predictions(lit_model, prompt)\n",
        "print(preds_str)"
      ],
      "metadata": {
        "id": "IXzeF7Z3FyYY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}